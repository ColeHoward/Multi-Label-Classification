{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e681ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as st\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29411ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Family</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraHylaedactylus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>Dendrobatidae</td>\n",
       "      <td>Ameerega</td>\n",
       "      <td>Ameeregatrivittata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Hypsiboas</td>\n",
       "      <td>HypsiboasCinerascens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraHylaedactylus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraHylaedactylus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Hypsiboas</td>\n",
       "      <td>HypsiboasCinerascens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Hypsiboas</td>\n",
       "      <td>HypsiboasCinerascens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Hypsiboas</td>\n",
       "      <td>HypsiboasCinerascens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Dendrobatidae</td>\n",
       "      <td>Ameerega</td>\n",
       "      <td>Ameeregatrivittata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Family      Genus                 Species\n",
       "2004  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
       "1194    Dendrobatidae   Ameerega      Ameeregatrivittata\n",
       "5359          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
       "1756  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
       "497   Leptodactylidae  Adenomera          AdenomeraAndre\n",
       "...               ...        ...                     ...\n",
       "3772  Leptodactylidae  Adenomera  AdenomeraHylaedactylus\n",
       "5191          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
       "5226          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
       "5390          Hylidae  Hypsiboas    HypsiboasCinerascens\n",
       "860     Dendrobatidae   Ameerega      Ameeregatrivittata\n",
       "\n",
       "[5036 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29#\n",
    "frogs_data = pd.read_csv('../data/Anuran Calls (MFCCs)/Frogs_MFCCs.csv')\n",
    "X = frogs_data.drop(['Family', 'Genus', 'Species', 'RecordID'], axis=1)\n",
    "y = frogs_data[['Family', 'Genus', 'Species']]\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851dea1",
   "metadata": {},
   "source": [
    "###### Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem. One of the most important approaches to multi-label classification is to train a classifier for each label (binary relevance).\n",
    "\n",
    "# 1\n",
    "## (b)\n",
    "### i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02e4be",
   "metadata": {},
   "source": [
    "https://mmuratarat.github.io/2020-01-25/multilabel_classification_metrics\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n",
    "###### <ins>Evaluating Multilabel Classification</ins>\n",
    "- **Exact Match Ratio**$ = \\frac{1}{n}\\sum_i^n{ I(y_i = \\hat{y}_i})$\n",
    "    - this metric is the ratio of correctly classed observations and total observations\n",
    "    - It makes no distinction between predictions that are partially correct and completely incorrect\n",
    "- **Hamming Loss** = $\\frac{1}{n}\\sum_i^n{\\sum_j^L{\\frac{1}{L}(I(y_i^j≠\\hat{y}_i^j}})$\n",
    "    - The inner summation is number of labels that were identified incorrectly\n",
    "        - meaning if an observation has 5 labels and you correctly identified 2, the hamming score is .6\n",
    "    - the outer summation averages the hamming score over all observations\n",
    "    - Range: [0, 1] where 0 is the optimal value\n",
    "- **Jaccard Index** = $\\frac{Y ∩ \\hat{Y}}{Y ∪ \\hat{Y}} = \\frac{TP}{TP+TN+FN}$\n",
    "    - the number of correctly predicted labels divided by the sum of the number of true labels and the number of predicted labels\n",
    "    - Thus, the best value is where $Y$ and $\\hat{Y}$ are the same, meaning the Jaccard Index is .5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fda2a",
   "metadata": {},
   "source": [
    "### i. <ins>Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db393206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validated Hamming Scores: \n",
      "          1         10        100       1000      10000\n",
      "0.001  0.091451  0.059642  0.034460  0.012591  0.011266\n",
      "0.010  0.042412  0.013254  0.007290  0.008615  0.008615\n",
      "1.000  0.098741  0.088138  0.088138  0.088138  0.088138\n",
      "\n",
      "Cross Validated Exact Match Scores: \n",
      "          1         10        100       1000      10000\n",
      "0.001  0.878728  0.910537  0.954274  0.982107  0.982107\n",
      "0.010  0.946322  0.978131  0.988072  0.990060  0.990060\n",
      "1.000  0.878728  0.888668  0.888668  0.888668  0.888668\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
    "\n",
    "gammas = [.001, .01, 1] # kernel widths\n",
    "penalties = [1, 10, 100, 1000, 10000] # penalty term scalars\n",
    "ham_c_gamma_df = pd.DataFrame(index=gammas, columns=penalties) # holds hamming loss for each gamma, C pair\n",
    "exact_c_gamma_df = pd.DataFrame(index=gammas, columns=penalties) # holds exact match score for each gamma, C pair\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=49)\n",
    "\n",
    "for c in penalties: # test indicated penalty scalars\n",
    "    avg_hams = []\n",
    "    avg_exact = []\n",
    "    for gamma in gammas: # test indicated kernel widths\n",
    "        \n",
    "        # 10-fold CV\n",
    "        for train_index, test_index in kf.split(X_train):\n",
    "            X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "            \n",
    "            hamming_scores = []\n",
    "            exact_scores = []\n",
    "            pred_df = pd.DataFrame(columns=y_train.columns, index=y_test_cv.index)\n",
    "            \n",
    "            # train SVM model for each output-label-column\n",
    "            for col in y_train.columns:\n",
    "                pipe = Pipeline(steps= [('scale', StandardScaler()), ('svm', SVC(gamma=gamma, kernel='rbf', C=c))])\n",
    "                ovr = OneVsRestClassifier(pipe, n_jobs=-1).fit(X_train_cv, y_train_cv[col])\n",
    "                \n",
    "                pred_df[col] = ovr.predict(X_test_cv)\n",
    "            \n",
    "            # calculate hamming loss\n",
    "            for i in y_test_cv.index:\n",
    "                ham_score = 0\n",
    "                if pred_df['Family'][i] == y_test_cv['Family'][i]:\n",
    "                    ham_score += 1\n",
    "                if pred_df['Genus'][i] == y_test_cv['Genus'][i]:\n",
    "                    ham_score += 1\n",
    "                if pred_df['Species'][i] == y_test_cv['Species'][i]:\n",
    "                    ham_score += 1\n",
    "                \n",
    "                exact_scores.append(1 if ham_score == 3 else 0)\n",
    "                hamming_scores.append(1 - (ham_score / 3))\n",
    "                \n",
    "        avg_exact.append(sum(exact_scores) / len(exact_scores))\n",
    "        avg_hams.append(sum(hamming_scores) / len(hamming_scores))\n",
    "        \n",
    "    ham_c_gamma_df[c] = avg_hams\n",
    "    exact_c_gamma_df[c] = avg_exact\n",
    "\n",
    "print('Cross Validated Hamming Scores: ')\n",
    "print(ham_c_gamma_df)\n",
    "print('\\nCross Validated Exact Match Scores: ')\n",
    "print(exact_c_gamma_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323d1f8",
   "metadata": {},
   "source": [
    "##### The Exact Match score maxes out when gamma = .01 and the penalty scalar is ≥ 1000. Interestingly, the Hamming loss prefers a smaller penalty and is lowest when C = 100.  \n",
    "\n",
    "Note that hamming score is just 1 - hamming loss (as per TA's comment on piazza post @1193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b77d7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.908549</td>\n",
       "      <td>0.940358</td>\n",
       "      <td>0.965540</td>\n",
       "      <td>0.987409</td>\n",
       "      <td>0.988734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.957588</td>\n",
       "      <td>0.986746</td>\n",
       "      <td>0.992710</td>\n",
       "      <td>0.991385</td>\n",
       "      <td>0.991385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.901259</td>\n",
       "      <td>0.911862</td>\n",
       "      <td>0.911862</td>\n",
       "      <td>0.911862</td>\n",
       "      <td>0.911862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         10        100       1000      10000\n",
       "0.001  0.908549  0.940358  0.965540  0.987409  0.988734\n",
       "0.010  0.957588  0.986746  0.992710  0.991385  0.991385\n",
       "1.000  0.901259  0.911862  0.911862  0.911862  0.911862"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hamming score\n",
    "ham_c_gamma_df.applymap(lambda x: 1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc94321",
   "metadata": {},
   "source": [
    "### iii. <ins>Re-train SVM with L1-penalty</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6c99eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross Validated Hamming Scores: \n",
      "C=10000; Hamming loss: 0.048376408217362485\n",
      "\n",
      "Cross Validated Exact Match Scores: \n",
      "C=10000; Exact Match Score: 0.9184890656063618\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "penalties = [10, 100, 1000, 10000] # penalty term scalars\n",
    "ham_c_df = pd.DataFrame(columns=penalties) # holds hamming loss for each gamma, C pair\n",
    "exact_c_df = pd.DataFrame(columns=penalties) # holds exact match score for each gamma, C pair\n",
    "kf = KFold(n_splits=10, random_state= 49, shuffle=True)\n",
    "\n",
    "for c in penalties: # test indicated penalty scalars\n",
    "    avg_hams = []\n",
    "    avg_exact = []\n",
    "        \n",
    "    # 10-fold CV\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        hamming_scores = []\n",
    "        exact_scores = []\n",
    "        pred_df = pd.DataFrame(columns=y_train.columns, index=y_test_cv.index)\n",
    "\n",
    "        # train SVM model for each output-label-column\n",
    "        for col in y_train.columns:\n",
    "            pipe = Pipeline(steps= [('scale', StandardScaler()), \n",
    "                                    ('svm', LinearSVC(penalty='l1', multi_class='ovr', max_iter=10000,\n",
    "                                                      C=c, loss='squared_hinge', dual=False))])\n",
    "            ovr = pipe.fit(X_train_cv, y_train_cv[col])\n",
    "\n",
    "            pred_df[col] = ovr.predict(X_test_cv)\n",
    "\n",
    "        # calculate hamming loss\n",
    "        for i in y_test_cv.index:\n",
    "            ham_score = 0\n",
    "            if pred_df['Family'][i] == y_test_cv['Family'][i]:\n",
    "                ham_score += 1\n",
    "            if pred_df['Genus'][i] == y_test_cv['Genus'][i]:\n",
    "                ham_score += 1\n",
    "            if pred_df['Species'][i] == y_test_cv['Species'][i]:\n",
    "                ham_score += 1\n",
    "\n",
    "            exact_scores.append(1 if ham_score == 3 else 0)\n",
    "            hamming_scores.append(1 - (ham_score / 3))\n",
    "\n",
    "    avg_exact.append(sum(exact_scores) / len(exact_scores))\n",
    "    avg_hams.append(sum(hamming_scores) / len(hamming_scores))\n",
    "\n",
    "ham_c_df[c] = avg_hams\n",
    "exact_c_df[c] = avg_exact\n",
    "\n",
    "print('Best Cross Validated Hamming Scores: ')\n",
    "print(f'C={penalties[np.nanargmin(ham_c_df.to_numpy())]}; Hamming loss: {ham_c_df[penalties[np.nanargmin(ham_c_df.to_numpy())]].to_numpy()[0]}')\n",
    "print('\\nCross Validated Exact Match Scores: ')\n",
    "print(f'C={penalties[np.nanargmax(exact_c_df.to_numpy())]}; Exact Match Score: {exact_c_df[penalties[np.nanargmax(exact_c_df.to_numpy())]].to_numpy()[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10ded6",
   "metadata": {},
   "source": [
    "### iii. <ins>by use SMOTE or any other method you know to remedy class imbalance and retrain SVM. Report your conclusions about the classifiers you trained</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b69e833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train_res_fam, y_train_fam = SMOTE(random_state=49, n_jobs=-1).fit_resample(X_train_cv, y_train_cv['Family'])\n",
    "# X_train_res_genus, y_train_genus = SMOTE(random_state=49, n_jobs=-1).fit_resample(X_train_cv, y_train_cv['Genus'])\n",
    "# X_train_res_genus, y_train_genus = SMOTE(random_state=49, n_jobs=-1).fit_resample(X_train_cv, y_train_cv['Species'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b5bc04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross Validated Hamming Scores: \n",
      "C=10000; Hamming loss: 0.06825712392312791ham_c_gamma_df\n",
      "\n",
      "Cross Validated Exact Match Scores: \n",
      "C=10000; Exact Match Score: 0.8628230616302187ham_c_gamma_df\n"
     ]
    }
   ],
   "source": [
    "penalties = [10, 100, 1000, 10000] # penalty term scalars\n",
    "ham_c_df = pd.DataFrame(columns=penalties) # holds hamming loss for each gamma, C pair\n",
    "exact_c_df = pd.DataFrame(columns=penalties) # holds exact match score for each gamma, C pair\n",
    "kf = KFold(n_splits=10, random_state= 49, shuffle=True)\n",
    "\n",
    "for c in penalties: # test indicated penalty scalars\n",
    "    avg_hams = []\n",
    "    avg_exact = []\n",
    "        \n",
    "    # 10-fold CV\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        hamming_scores = []\n",
    "        exact_scores = []\n",
    "        pred_df = pd.DataFrame(columns=y_train.columns, index=y_test_cv.index)\n",
    "\n",
    "        # train SVM model for each output-label-column\n",
    "        for col in y_train.columns:\n",
    "            X_train_res, y_train_res = SMOTE(random_state=49, n_jobs=-1).fit_resample(X_train_cv, y_train_cv[col])\n",
    "            pipe = Pipeline(steps = [('scale', StandardScaler()),\n",
    "                                    ('svm', LinearSVC(penalty='l1', multi_class='ovr', max_iter=100000,\n",
    "                                                      C=c, loss='squared_hinge', dual=False))])\n",
    "            ovr = pipe.fit(X_train_res, y_train_res)\n",
    "\n",
    "            pred_df[col] = ovr.predict(X_test_cv)\n",
    "\n",
    "        # calculate hamming loss\n",
    "        for i in y_test_cv.index:\n",
    "            ham_score = 0\n",
    "            if pred_df['Family'][i] == y_test_cv['Family'][i]:\n",
    "                ham_score += 1\n",
    "            if pred_df['Genus'][i] == y_test_cv['Genus'][i]:\n",
    "                ham_score += 1\n",
    "            if pred_df['Species'][i] == y_test_cv['Species'][i]:\n",
    "                ham_score += 1\n",
    "\n",
    "            exact_scores.append(1 if ham_score == 3 else 0)\n",
    "            hamming_scores.append(1 - (ham_score / 3))\n",
    "\n",
    "    avg_exact.append(sum(exact_scores) / len(exact_scores))\n",
    "    avg_hams.append(sum(hamming_scores) / len(hamming_scores))\n",
    "\n",
    "ham_c_df[c] = avg_hams\n",
    "exact_c_df[c] = avg_exact\n",
    "\n",
    "print('Best Cross Validated Hamming Scores: ')\n",
    "print(f'C={penalties[np.nanargmin(ham_c_df.to_numpy())]}; Hamming loss: {ham_c_df[penalties[np.nanargmin(ham_c_df.to_numpy())]].to_numpy()[0]}ham_c_gamma_df')\n",
    "print('\\nCross Validated Exact Match Scores: ')\n",
    "print(f'C={penalties[np.nanargmax(exact_c_df.to_numpy())]}; Exact Match Score: {exact_c_df[penalties[np.nanargmax(exact_c_df.to_numpy())]].to_numpy()[0]}ham_c_gamma_df')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec387470",
   "metadata": {},
   "source": [
    "# 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bb42754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m silhouette_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     km \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     labels \u001b[38;5;241m=\u001b[39m km\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     19\u001b[0m     silhouette_scores\u001b[38;5;241m.\u001b[39mappend(silhouette_score(X, labels))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1179\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1175\u001b[0m best_inertia, best_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init):\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m     centers_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_centroids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1090\u001b[0m, in \u001b[0;36mKMeans._init_centroids\u001b[0;34m(self, X, x_squared_norms, init, random_state, init_size)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1090\u001b[0m     centers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_kmeans_plusplus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1097\u001b[0m     seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mpermutation(n_samples)[:n_clusters]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:218\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[0;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m    215\u001b[0m np\u001b[38;5;241m.\u001b[39mclip(candidate_ids, \u001b[38;5;28;01mNone\u001b[39;00m, closest_dist_sq\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39mcandidate_ids)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Compute distances to center candidates\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[1;32m    223\u001b[0m np\u001b[38;5;241m.\u001b[39mminimum(closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:371\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    368\u001b[0m     distances \u001b[38;5;241m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX\n\u001b[1;32m    373\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/extmath.py:153\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    156\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m ):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\n",
    "# https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
    "# https://math.ryerson.ca/~danziger/professor/MTH108/Handouts/codes.pdf\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "ham_distances = []\n",
    "ham_scores = []\n",
    "ham_losses = []\n",
    "\n",
    "for simulation in range(50):\n",
    "    \n",
    "    # (a) choose best K automatically by sillhoutte score\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in range(2, 51):\n",
    "        km = KMeans(n_clusters = k, random_state=simulation).fit(X)\n",
    "        labels = km.labels_\n",
    "        silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "    best_k = np.argmax(silhouette_scores) + 2\n",
    "\n",
    "\n",
    "    # (b) In each cluster, determine which class is the majority by reading the true labels for family, genus, species\n",
    "    km = KMeans(n_clusters=best_k, random_state=simulation).fit(X)\n",
    "\n",
    "    clusters = km.labels_\n",
    "    family_labels  = y['Family'].unique()\n",
    "    genus_labels   = y['Genus'].unique()\n",
    "    species_labels = y['Species'].unique()\n",
    "\n",
    "    col_to_label_to_freq = {'Family': dict(zip(family_labels,  len(family_labels)*[0]) ), \n",
    "                           'Genus':   dict(zip(genus_labels,   len(genus_labels)*[0]) ),\n",
    "                           'Species': dict(zip(species_labels, len(species_labels)*[0]) )}\n",
    "\n",
    "    cluster_to_col_to_majClass = {}\n",
    "    cluster_to_col_to_label_to_freq = {}\n",
    "    for i in range(0, best_k):\n",
    "        cluster_to_col_to_label_to_freq[i] = col_to_label_to_freq\n",
    "        cluster_to_col_to_majClass[i] = dict(zip(['Family', 'Genus', 'Species'], ['', '', '']))\n",
    "\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_to_col_to_label_to_freq[cluster]['Family'][y['Family'][i]] += 1\n",
    "        cluster_to_col_to_label_to_freq[cluster]['Genus'][y['Genus'][i]] += 1\n",
    "        cluster_to_col_to_label_to_freq[cluster]['Species'][y['Species'][i]] += 1\n",
    "\n",
    "    for cluster, column_to_label_to_freq in cluster_to_col_to_label_to_freq.items():\n",
    "        for col, label_to_freq in column_to_label_to_freq.items():\n",
    "            \n",
    "            labels = list(label_to_freq.keys())\n",
    "            frequencies = list(label_to_freq.values())\n",
    "\n",
    "            majority_class = labels[frequencies.index(max(frequencies))]\n",
    "            cluster_to_col_to_majClass[cluster][col] = majority_class\n",
    "\n",
    "    # (c)\n",
    "    tot_ham_distance, tot_ham_score, tot_ham_loss = 0, 0, 0\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        ham = 0\n",
    "        if cluster_to_col_to_majClass[cluster]['Family'] == y['Family'][i]:\n",
    "            ham += 1\n",
    "        if cluster_to_col_to_majClass[cluster]['Genus'] == y['Genus'][i]:\n",
    "            ham += 1\n",
    "        if cluster_to_col_to_majClass[cluster]['Species'] == y['Species'][i]:\n",
    "            ham += 1\n",
    "\n",
    "        tot_ham_distance += (3 - ham)\n",
    "        tot_ham_score += (ham / 3)\n",
    "        tot_ham_loss += (1 - (ham/3))\n",
    "\n",
    "\n",
    "    avg_ham_distance = tot_ham_distance / len(clusters)\n",
    "    avg_ham_score = tot_ham_score / len(clusters)\n",
    "    avg_ham_loss = tot_ham_loss / len(clusters)\n",
    "\n",
    "    ham_distances.append(avg_ham_distance)\n",
    "    ham_scores.append(avg_ham_score)\n",
    "    ham_losses.append(avg_ham_loss)\n",
    "\n",
    "print(f'Hamming Score-- avg: {np.average(ham_scores)}; std. dev.: {np.std(ham_scores)}')\n",
    "print(f'Hamming Loss-- avg: {np.average(ham_losses)}; std. dev.: {np.std(ham_losses)}')\n",
    "print(f'Hamming Distance-- avg: {np.average(ham_distances)}; std. dev.: {np.std(ham_distances)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02125351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7195\n",
      "8622\n"
     ]
    }
   ],
   "source": [
    "print(len(km.labels_))\n",
    "print(sum(km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864491cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef50a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71b1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e4b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bedb3197",
   "metadata": {},
   "source": [
    "# 3\n",
    "### <ins>ISLR: 12.6.2</ins>\n",
    "Suppose that we have four observations, for which we compute a dissimilarity matrix, given by:\n",
    "\n",
    "$$\\begin{pmatrix}& .3 & .4 & .7\\\\.3 & & .5 & .8\\\\.4 & .5 & & .45\\\\.7 & .8 & .45\\end{pmatrix}$$\n",
    "\n",
    "For instance, the dissimilarity between the first and second observations is 0.3, and the dissimilarity between the second and fourth observations is 0.8.\n",
    "\n",
    "(a) On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observa- tions using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram.\n",
    "- answer\n",
    "\n",
    "(b) Repeat (a), this time using single linkage clustering.\n",
    "- answer\n",
    "\n",
    "(c) Suppose that we cut the dendrogram obtained in (a) such that two clusters result. Which observations are in each cluster?\n",
    "- answer\n",
    "\n",
    "(d) Suppose that we cut the dendrogram obtained in (b) such that two clusters result. Which observations are in each cluster?\n",
    "- answer\n",
    "\n",
    "(e) It is mentioned in the chapter that at each fusion in the dendrogram, the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram. Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same.\n",
    "- answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f2ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946c28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
